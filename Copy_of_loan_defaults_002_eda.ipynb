{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOrOeS15138zTaCu8IeWEyV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/loan_defaults/blob/main/Copy_of_loan_defaults_002_eda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Description & Preprocessing Steps\n",
        "\n",
        "This dataset contains information on clients' credit card behavior, provided by a financial institution in Taiwan. The target variable is `default_payment_next_month`, which indicates whether the client defaulted on their credit card payment the next month.\n",
        "\n",
        "url = https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients\n",
        "\n",
        "#### Variables:\n",
        "\n",
        "- **ID**: ID of each client.\n",
        "- **LIMIT_BAL**: Amount of given credit in NT dollars (includes individual and family/supplementary credit).\n",
        "- **SEX**: Gender (1 = male, 2 = female).\n",
        "- **EDUCATION**: Education level (1 = graduate school, 2 = university, 3 = high school, 4 = others, 5 = unknown, 6 = unknown).\n",
        "- **MARRIAGE**: Marital status (1 = married, 2 = single, 3 = others).\n",
        "- **AGE**: Age in years.\n",
        "\n",
        "#### Payment History (PAY_X):\n",
        "- **PAY_0**: Repayment status in September 2005 (-1 = pay duly, 1 = payment delay for one month, 2 = payment delay for two months, ... 8 = payment delay for eight months, 9 = payment delay for nine months and above).\n",
        "- **PAY_2**: Repayment status in August 2005.\n",
        "- **PAY_3**: Repayment status in July 2005.\n",
        "- **PAY_4**: Repayment status in June 2005.\n",
        "- **PAY_5**: Repayment status in May 2005.\n",
        "- **PAY_6**: Repayment status in April 2005.\n",
        "\n",
        "#### Bill Statement Amount (BILL_AMT_X):\n",
        "- **BILL_AMT1**: Amount of bill statement in September 2005 (NT dollars).\n",
        "- **BILL_AMT2**: Amount of bill statement in August 2005 (NT dollars).\n",
        "- **BILL_AMT3**: Amount of bill statement in July 2005 (NT dollars).\n",
        "- **BILL_AMT4**: Amount of bill statement in June 2005 (NT dollars).\n",
        "- **BILL_AMT5**: Amount of bill statement in May 2005 (NT dollars).\n",
        "- **BILL_AMT6**: Amount of bill statement in April 2005 (NT dollars).\n",
        "\n",
        "#### Previous Payment Amount (PAY_AMT_X):\n",
        "- **PAY_AMT1**: Amount of previous payment in September 2005 (NT dollars).\n",
        "- **PAY_AMT2**: Amount of previous payment in August 2005 (NT dollars).\n",
        "- **PAY_AMT3**: Amount of previous payment in July 2005 (NT dollars).\n",
        "- **PAY_AMT4**: Amount of previous payment in June 2005 (NT dollars).\n",
        "- **PAY_AMT5**: Amount of previous payment in May 2005 (NT dollars).\n",
        "- **PAY_AMT6**: Amount of previous payment in April 2005 (NT dollars).\n",
        "\n",
        "- **default_payment_next_month**: Default payment indicator (1 = yes, 0 = no).\n",
        "\n",
        "#### Explanation for Feature Reordering:\n",
        "The bill statement and payment amounts are listed in reverse chronological order in the dataset. To ensure that the feature names match the actual sequence of events, we reverse the column names for `BILL_AMT` and `PAY_AMT` features so that they correctly represent the time sequence from April 2005 to September 2005.\n",
        "\n",
        "\n",
        "### Data Preprocessing Steps\n",
        "\n",
        "1. **Reorder and Rename the Bill and Payment Features**:\n",
        "   - The `BILL_AMT` and `PAY_AMT` features are currently listed in reverse chronological order (September 2005 to April 2005). We will reorder these columns to match the actual timeline from April 2005 to September 2005, and rename them with more descriptive labels such as `bill_amt_9_september` and `pay_amt_9_september` to clearly indicate the time period they represent.\n",
        "   \n",
        "2. **Reorder and Rename the Payment Delay Features**:\n",
        "   - The `PAY_X` columns represent the payment delay status for each month. These features are ordinal in nature, as a higher number indicates a longer delay and higher risk of default. We will rename these columns to reflect the delay and the corresponding month, using a format like `pay_delay_9_september` for consistency and clarity.\n",
        "\n",
        "3. **Convert Categorical Variables to Human-Readable Text**:\n",
        "   - The categorical variables such as `SEX`, `EDUCATION`, and `MARRIAGE` are currently represented by numeric codes (e.g., 1, 2, 3). We will convert these codes to their corresponding text labels for better interpretability during analysis:\n",
        "     - `SEX`: Convert 1 to \"Male\" and 2 to \"Female\".\n",
        "     - `EDUCATION`: Convert the numeric codes to \"Graduate School\", \"University\", \"High School\", etc.\n",
        "     - `MARRIAGE`: Convert the numeric codes to \"Married\", \"Single\", etc.\n",
        "\n",
        "4. **Remove the `ID` Column**:\n",
        "   - The `ID` column is simply an identifier and does not provide useful information for analysis. We will drop this column from the dataset.\n",
        "\n",
        "5. **Ensure Correct Column Ordering**:\n",
        "   - To maintain consistency and ease of analysis, we will keep the most important features (`limit_bal`, `sex`, `education`, `marriage`, `age`) at the beginning of the dataframe, followed by the renamed and reordered bill, payment, and delay columns.\n",
        "\n",
        "6. **Acknowledge the Ordinal Nature of Payment Delay Features**:\n",
        "   - The payment delay features (e.g., `pay_delay_9_september`) are ordinal and represent an increasing level of risk as the delay period extends. This ordinal relationship will be preserved during preprocessing and modeling.\n"
      ],
      "metadata": {
        "id": "NjJGNa0zVRt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yiFvXx3_XFeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loadm Clean & Preprocess Data"
      ],
      "metadata": {
        "id": "iq5wllY6e3eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from loan_data_utils import load_and_preprocess_data, primary_columns, bill_columns_order, pay_columns, ordinal_columns, check_categorical_order\n",
        "from eda_utils import data_overview, plot_univariate_distributions\n",
        "import logging\n",
        "\n",
        "# Define your URL, categorical columns, and target\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
        "categorical_columns = ['sex', 'marriage']\n",
        "target = 'default_payment_next_month'\n",
        "\n",
        "# Load and preprocess data\n",
        "X, y = load_and_preprocess_data(url, categorical_columns, target)\n",
        "\n",
        "# print data overview\n",
        "data_overview(X)\n",
        "\n",
        "# Drop duplicate rows\n",
        "X = X.drop_duplicates()\n",
        "\n",
        "# Confirm the duplicates are removed\n",
        "print(f\"Number of Duplicate Rows After Dropping: {X.duplicated().sum()}\")\n",
        "\n",
        "# List of columns to check\n",
        "categorical_columns = ['education', 'sex', 'marriage']  # Add more columns if needed\n",
        "pay_cols = [col for col in X.columns if col.startswith('pay_delay')]\n",
        "# Check the order of categorical columns\n",
        "check_categorical_order(X, categorical_columns + pay_cols)\n"
      ],
      "metadata": {
        "id": "6HxbQMjvTcYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ca6213-bd0a-4ba4-83b5-7e9113b906f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing Values: 0\n",
            "Missing Percentage: 0.0\n",
            "Number of Duplicate Rows: 56\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30000 entries, 0 to 29999\n",
            "Data columns (total 23 columns):\n",
            " #   Column                 Non-Null Count  Dtype   \n",
            "---  ------                 --------------  -----   \n",
            " 0   limit_bal              30000 non-null  int64   \n",
            " 1   sex                    30000 non-null  category\n",
            " 2   education              30000 non-null  category\n",
            " 3   marriage               30000 non-null  category\n",
            " 4   age                    30000 non-null  int64   \n",
            " 5   bill_amt_4_april       30000 non-null  int64   \n",
            " 6   bill_amt_5_may         30000 non-null  int64   \n",
            " 7   bill_amt_6_june        30000 non-null  int64   \n",
            " 8   bill_amt_7_july        30000 non-null  int64   \n",
            " 9   bill_amt_8_august      30000 non-null  int64   \n",
            " 10  bill_amt_9_september   30000 non-null  int64   \n",
            " 11  pay_amt_4_april        30000 non-null  int64   \n",
            " 12  pay_amt_5_may          30000 non-null  int64   \n",
            " 13  pay_amt_6_june         30000 non-null  int64   \n",
            " 14  pay_amt_7_july         30000 non-null  int64   \n",
            " 15  pay_amt_8_august       30000 non-null  int64   \n",
            " 16  pay_amt_9_september    30000 non-null  int64   \n",
            " 17  pay_delay_9_september  30000 non-null  category\n",
            " 18  pay_delay_8_august     30000 non-null  category\n",
            " 19  pay_delay_7_july       30000 non-null  category\n",
            " 20  pay_delay_6_june       30000 non-null  category\n",
            " 21  pay_delay_5_may        30000 non-null  category\n",
            " 22  pay_delay_4_april      30000 non-null  category\n",
            "dtypes: category(9), int64(14)\n",
            "memory usage: 3.5 MB\n",
            "Number of Duplicate Rows After Dropping: 0\n",
            "Column: education\n",
            "Categories: Index(['Other/Unknown', 'High School', 'University', 'Graduate School'], dtype='object')\n",
            "Ordered: True\n",
            "\n",
            "Column: sex\n",
            "Categories: Index(['Female', 'Male'], dtype='object')\n",
            "Ordered: False\n",
            "\n",
            "Column: marriage\n",
            "Categories: Index(['Married', 'Single', 'Unknown/Others'], dtype='object')\n",
            "Ordered: False\n",
            "\n",
            "Column: pay_delay_9_september\n",
            "Categories: Index(['No consumption', 'Paid in full', 'Revolving credit', '1 month delay',\n",
            "       '2 months delay', '3 months delay', '4 months delay', '5 months delay',\n",
            "       '6 months delay', '7 months delay', '8 months delay',\n",
            "       '9+ months delay'],\n",
            "      dtype='object')\n",
            "Ordered: True\n",
            "\n",
            "Column: pay_delay_8_august\n",
            "Categories: Index(['No consumption', 'Paid in full', 'Revolving credit', '1 month delay',\n",
            "       '2 months delay', '3 months delay', '4 months delay', '5 months delay',\n",
            "       '6 months delay', '7 months delay', '8 months delay',\n",
            "       '9+ months delay'],\n",
            "      dtype='object')\n",
            "Ordered: True\n",
            "\n",
            "Column: pay_delay_7_july\n",
            "Categories: Index(['No consumption', 'Paid in full', 'Revolving credit', '1 month delay',\n",
            "       '2 months delay', '3 months delay', '4 months delay', '5 months delay',\n",
            "       '6 months delay', '7 months delay', '8 months delay',\n",
            "       '9+ months delay'],\n",
            "      dtype='object')\n",
            "Ordered: True\n",
            "\n",
            "Column: pay_delay_6_june\n",
            "Categories: Index(['No consumption', 'Paid in full', 'Revolving credit', '1 month delay',\n",
            "       '2 months delay', '3 months delay', '4 months delay', '5 months delay',\n",
            "       '6 months delay', '7 months delay', '8 months delay',\n",
            "       '9+ months delay'],\n",
            "      dtype='object')\n",
            "Ordered: True\n",
            "\n",
            "Column: pay_delay_5_may\n",
            "Categories: Index(['No consumption', 'Paid in full', 'Revolving credit', '1 month delay',\n",
            "       '2 months delay', '3 months delay', '4 months delay', '5 months delay',\n",
            "       '6 months delay', '7 months delay', '8 months delay',\n",
            "       '9+ months delay'],\n",
            "      dtype='object')\n",
            "Ordered: True\n",
            "\n",
            "Column: pay_delay_4_april\n",
            "Categories: Index(['No consumption', 'Paid in full', 'Revolving credit', '1 month delay',\n",
            "       '2 months delay', '3 months delay', '4 months delay', '5 months delay',\n",
            "       '6 months delay', '7 months delay', '8 months delay',\n",
            "       '9+ months delay'],\n",
            "      dtype='object')\n",
            "Ordered: True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Understand the Data Structure**\n",
        "   - **Inspect Data Types:** Identify which features are categorical and which are numeric. This will help decide the appropriate visualizations and analyses.\n",
        "   - **Check for Missing Values:** Look for missing data and determine whether you need to fill, drop, or otherwise handle these missing values.\n",
        "   - **Check for Duplicates:** Identify and handle any duplicate records if necessary."
      ],
      "metadata": {
        "id": "dt0FuMRbEc1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[col for col in X.columns if col.startswith('pay_amt')]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3y4F1DZDhMz",
        "outputId": "f9d6edbd-a380-4c94-d010-6c9f81549934"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pay_amt_4_april',\n",
              " 'pay_amt_5_may',\n",
              " 'pay_amt_6_june',\n",
              " 'pay_amt_7_july',\n",
              " 'pay_amt_8_august',\n",
              " 'pay_amt_9_september']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns + ordinal_columns"
      ],
      "metadata": {
        "id": "x5FrqOh-cTIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "To explore and visualize your loan default data, you can break down the process into the following steps. This strategy focuses on understanding the distribution, relationships, and key patterns within both categorical and numeric features. Here’s a structured approach:\n",
        "\n",
        "### 1. **Understand the Data Structure**\n",
        "   - **Inspect Data Types:** Identify which features are categorical and which are numeric. This will help decide the appropriate visualizations and analyses.\n",
        "   - **Check for Missing Values:** Look for missing data and determine whether you need to fill, drop, or otherwise handle these missing values.\n",
        "   - **Check for Duplicates:** Identify and handle any duplicate records if necessary.\n",
        "\n",
        "### 2. **Distribution of Features**\n",
        "   - **Univariate Analysis:**\n",
        "     - For numeric features: Plot histograms, boxplots, and KDE plots to understand the distribution, outliers, and skewness.\n",
        "     - For categorical features: Use bar plots or count plots to see the frequency distribution of each category.\n",
        "\n",
        "### 3. **Explore Relationships Between Features**\n",
        "   - **Bivariate Analysis:**\n",
        "     - Numeric vs Numeric: Use scatter plots or pair plots (seaborn’s `pairplot`) to examine relationships between numeric features.\n",
        "     - Categorical vs Numeric: Use box plots or violin plots to understand how numeric features vary across different categories.\n",
        "     - Categorical vs Categorical: Use stacked bar charts or heatmaps to explore relationships between categorical features.\n",
        "\n",
        "### 4. **Analyze Correlations**\n",
        "   - **Correlation Matrix:** Compute and visualize the correlation matrix for numeric features using a heatmap. Pay attention to multicollinearity and strong correlations.\n",
        "   - **Correlation with the Target Variable:** Focus on identifying features that are strongly correlated with the target variable (e.g., loan default status). This will guide feature selection.\n",
        "\n",
        "### 5. **Outliers and Anomalies**\n",
        "   - **Boxplots and Z-Scores:** Use boxplots and calculate Z-scores to detect and visualize outliers in numeric features.\n",
        "   - **Targeted Outlier Detection:** Focus on outliers in features that are critical for predicting loan defaults.\n",
        "\n",
        "### 6. **Target Variable Distribution**\n",
        "   - **Class Balance:** If the target is binary (e.g., default/no default), plot the class distribution using a count plot or pie chart to assess class imbalance.\n",
        "\n",
        "### 7. **Feature Interactions and Grouping**\n",
        "   - **Interaction Plots:** For combinations of categorical features or combinations of categorical and numeric features, use interaction plots or grouped box plots.\n",
        "   - **Pivot Tables or Grouped Aggregations:** Summarize numeric data by categories to observe trends, such as mean or median loan balances across different education levels.\n",
        "\n",
        "### 8. **Feature Engineering Insights**\n",
        "   - Identify potential new features based on the EDA results, such as ratios, binning numeric features, or interaction terms (e.g., combining income with payment history).\n",
        "\n",
        "### 9. **Dimensionality Reduction (Optional for Visualization)**\n",
        "   - **PCA or t-SNE:** Reduce the dimensionality of numeric data to visualize patterns and clusters. This can help in understanding the structure of the dataset.\n",
        "\n",
        "### 10. **Final Summary and Reporting**\n",
        "   - Summarize your findings, noting key patterns, correlations, and potential challenges like multicollinearity, outliers, or class imbalance.\n",
        "   - Decide on the preprocessing steps needed based on your analysis (e.g., transformations, scaling, encoding).\n",
        "\n",
        "### Visualization Summary:\n",
        "- **Histograms, Boxplots, KDE Plots:** For distribution of numeric data.\n",
        "- **Count Plots, Bar Plots:** For categorical data.\n",
        "- **Scatter Plots, Pair Plots:** For numeric feature interactions.\n",
        "- **Heatmaps:** For correlation analysis.\n",
        "- **Class Balance Plots:** For target distribution.\n",
        "- **Interaction Plots:** For combinations of categorical and numeric features.\n",
        "\n"
      ],
      "metadata": {
        "id": "TXrzLe3pCvly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. **Distribution of Features**\n",
        "   - **Univariate Analysis:**\n",
        "     - For numeric features: Plot histograms, boxplots, and KDE plots to understand the distribution, outliers, and skewness.\n",
        "     - For categorical features: Use bar plots or count plots to see the frequency distribution of each category.\n"
      ],
      "metadata": {
        "id": "tGW9ujFZI6zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from eda_utils import data_overview, plot_class_distribution, plot_univariate_distributions\n",
        "\n",
        "# Visualize the distributions\n",
        "plot_univariate_distributions(X, categorical_columns + ordinal_columns)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YI0j_W-lHvHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pay as Category\n",
        "\n",
        "-2: No consumption; -1: Paid in full; 0: The use of revolving credit; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above."
      ],
      "metadata": {
        "id": "hMIkzyN-PUV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X['pay_delay_9_september'].value_counts()\n",
        "#"
      ],
      "metadata": {
        "id": "ybwqdVb-HvDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[X['marriage'] == 0]"
      ],
      "metadata": {
        "id": "RORnWkUZHvAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BcMJgWv6Hu7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling outliers can be done either inside an sklearn preprocessing pipeline or before passing the data to the pipeline. Here’s a breakdown of the two approaches to help you decide:\n",
        "\n",
        "### 1. **Handling Outliers Inside an sklearn Pipeline**:\n",
        "This approach integrates outlier handling as part of your preprocessing steps within the pipeline. It allows you to maintain a streamlined and consistent workflow where all data transformations are done within a single pipeline.\n",
        "\n",
        "**Pros**:\n",
        "- **Consistency**: Everything is handled in one place, making the pipeline reusable and easier to manage.\n",
        "- **Modularity**: You can easily switch out or adjust outlier handling without affecting other preprocessing steps.\n",
        "\n",
        "**Cons**:\n",
        "- **Complexity**: Implementing custom outlier handling inside a pipeline might require creating custom transformers, which adds complexity.\n",
        "- **Performance**: If your outlier detection is computationally intensive, it could slow down the pipeline.\n",
        "\n",
        "**How to Implement It**:\n",
        "You can use a custom transformer to handle outliers within the pipeline:\n",
        "```python\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strategy='clip', lower_percentile=0.01, upper_percentile=0.99):\n",
        "        self.strategy = strategy\n",
        "        self.lower_percentile = lower_percentile\n",
        "        self.upper_percentile = upper_percentile\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.strategy == 'clip':\n",
        "            self.lower_bound = X.quantile(self.lower_percentile)\n",
        "            self.upper_bound = X.quantile(self.upper_percentile)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.strategy == 'clip':\n",
        "            X = np.clip(X, self.lower_bound, self.upper_bound)\n",
        "        return X\n",
        "\n",
        "# Usage in a pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('outlier_handler', OutlierHandler(strategy='clip')),\n",
        "    # Add other preprocessing steps here\n",
        "])\n",
        "```\n",
        "\n",
        "### 2. **Handling Outliers Before Passing Data to the Pipeline**:\n",
        "In this approach, you detect and handle outliers in a separate preprocessing step before the data enters the sklearn pipeline. This gives you more flexibility and control over the outlier handling process.\n",
        "\n",
        "**Pros**:\n",
        "- **Flexibility**: You have more freedom to use advanced outlier detection methods (e.g., IQR, Z-score, Isolation Forest) without being constrained by the pipeline structure.\n",
        "- **Easier Debugging**: Handling outliers outside the pipeline makes it easier to inspect and understand the changes before they go through the rest of the preprocessing steps.\n",
        "\n",
        "**Cons**:\n",
        "- **Inconsistent Workflow**: Having preprocessing steps outside the pipeline can make your workflow harder to manage and less reusable.\n",
        "- **Reproducibility**: If outlier handling is done outside the pipeline, it might be harder to reproduce or apply the same steps consistently across different datasets.\n",
        "\n",
        "**How to Implement It**:\n",
        "You could handle outliers using a separate function:\n",
        "```python\n",
        "def handle_outliers(df, lower_percentile=0.01, upper_percentile=0.99):\n",
        "    lower_bound = df.quantile(lower_percentile)\n",
        "    upper_bound = df.quantile(upper_percentile)\n",
        "    df = df.clip(lower_bound, upper_bound, axis=1)\n",
        "    return df\n",
        "\n",
        "# Usage before passing to the pipeline\n",
        "X = handle_outliers(X)\n",
        "```\n",
        "\n",
        "### Recommendation:\n",
        "- **If your goal is to keep everything modular and within a consistent pipeline**, handling outliers inside the sklearn pipeline is the better approach.\n",
        "- **If you prefer more flexibility and want to apply advanced outlier detection methods**, handling outliers before passing the data to the pipeline might be more suitable.\n",
        "\n",
        "**My Recommendation**: Start by handling outliers within the pipeline using a custom transformer. This keeps your workflow consistent and allows you to experiment with different strategies without needing to change your overall setup."
      ],
      "metadata": {
        "id": "LfciAg5PB8ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You’re asking a very important question. When handling outliers inside an sklearn pipeline, you need to be cautious about how this impacts your test data. Here’s how it works and the implications:\n",
        "\n",
        "### How Outlier Handling in a Pipeline Works:\n",
        "If you handle outliers within an sklearn pipeline, the transformations (like clipping, removing, or scaling) are **fitted on the training data** and then **applied to the test data** during the `.transform()` step.\n",
        "\n",
        "For example:\n",
        "1. **During Training**:\n",
        "   - The pipeline calculates the bounds (e.g., lower and upper limits) based on the training data.\n",
        "   - It applies these bounds to the training data.\n",
        "\n",
        "2. **During Testing**:\n",
        "   - The same bounds (calculated from the training data) are applied to the test data without recalculating them.\n",
        "\n",
        "### Implications of Removing Outliers in the Test Data:\n",
        "1. **Risk of Data Leakage**:\n",
        "   - If outliers are identified and removed based on the **entire dataset** (including test data), it leads to data leakage because information from the test set influences the model during training.\n",
        "   - In a pipeline, the outlier handling step should only use information from the training data.\n",
        "\n",
        "2. **Generalization Issues**:\n",
        "   - If you apply the same outlier handling strategy to the test data (e.g., clipping), it could change the nature of the test data, making it inconsistent with real-world data, which might still have outliers.\n",
        "   - For example, if your pipeline clips values in the test data based on training data bounds, the test data distribution might not reflect real-world scenarios.\n",
        "\n",
        "3. **Bias in Model Evaluation**:\n",
        "   - If your model never sees certain types of extreme values during training due to outlier removal, it might struggle to generalize when those extreme values appear in the real world.\n",
        "\n",
        "### Best Practices for Outlier Handling in a Pipeline:\n",
        "1. **Fit the Outlier Handling Only on Training Data**:\n",
        "   - The outlier detection/removal step should be **fitted on the training data only** and applied consistently to both the training and test sets.\n",
        "   - The `.fit()` step of the pipeline should calculate outlier thresholds (e.g., using IQR or quantiles) based only on the training data.\n",
        "\n",
        "2. **Avoid Removing Outliers from the Test Set**:\n",
        "   - You generally don’t want to completely remove outliers from the test set. Instead, use strategies like clipping or transforming extreme values to make the model robust while preserving the test data distribution.\n",
        "\n",
        "3. **Understand Real-World Implications**:\n",
        "   - If your model is deployed in a setting where outliers are common (e.g., finance), removing or transforming them can limit the model’s ability to handle such cases effectively. Be sure your outlier strategy aligns with your deployment environment.\n",
        "\n",
        "### Example Implementation in a Pipeline:\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import numpy as np\n",
        "\n",
        "class OutlierHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, strategy='clip', lower_percentile=0.01, upper_percentile=0.99):\n",
        "        self.strategy = strategy\n",
        "        self.lower_percentile = lower_percentile\n",
        "        self.upper_percentile = upper_percentile\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.strategy == 'clip':\n",
        "            self.lower_bound = X.quantile(self.lower_percentile)\n",
        "            self.upper_bound = X.quantile(self.upper_percentile)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.strategy == 'clip':\n",
        "            X = np.clip(X, self.lower_bound, self.upper_bound)\n",
        "        return X\n",
        "\n",
        "# Use in a pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('outlier_handler', OutlierHandler(strategy='clip')),\n",
        "    # Add other preprocessing steps here\n",
        "    # ('scaler', StandardScaler()),\n",
        "    # ('model', SomeModel())\n",
        "])\n",
        "```\n",
        "\n",
        "### Conclusion:\n",
        "- **Handling outliers inside the pipeline** allows you to maintain a consistent workflow where the outlier strategy is applied uniformly during both training and testing.\n",
        "- **Fit on training data only**: Always fit the outlier thresholds on the training data to avoid data leakage and ensure that the test data remains representative of real-world scenarios.\n",
        "- **Avoid removing test data outliers**: Instead, consider transforming or clipping them to preserve the test set distribution.\n",
        "\n",
        "By following these practices, you can avoid common pitfalls and ensure your model generalizes well."
      ],
      "metadata": {
        "id": "sx6yQM9dC1_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EDA Utils"
      ],
      "metadata": {
        "id": "ONYYdNCIMlxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_content=r'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#--------   Data Overview   --------#\n",
        "\n",
        "def data_overview(X):\n",
        "    # Calculate missing values and their percentage\n",
        "    missing_values = X.isnull().sum().sum()\n",
        "    missing_percentage = (missing_values / len(X)) * 100\n",
        "\n",
        "    # Calculate the number of duplicate rows\n",
        "    duplicate_rows = X.duplicated().sum()\n",
        "\n",
        "    # Print missing values, missing percentage, and duplicate information\n",
        "    print(\"Missing Values:\", missing_values)\n",
        "    print(\"Missing Percentage:\", missing_percentage)\n",
        "    print(f\"Number of Duplicate Rows: {duplicate_rows}\\n\")\n",
        "\n",
        "    # Print basic information\n",
        "    X.info()\n",
        "\n",
        "#--------   Plot Class Distribution   --------#\n",
        "\n",
        "def plot_class_distribution(y_train, target_name):\n",
        "    sns.set()\n",
        "    sns.set(style=\"ticks\")\n",
        "    sns.set_context('notebook')\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.countplot(x=y_train, hue=y_train, palette='mako')\n",
        "    plt.title(f'Class Distribution in Training Set: {target_name}')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend([], [], frameon=False)\n",
        "\n",
        "    # Calculate the percentage for each class\n",
        "    total = len(y_train)\n",
        "    class_counts = y_train.value_counts()\n",
        "    for i, count in enumerate(class_counts):\n",
        "        percentage = 100 * count / total\n",
        "        plt.text(i, count, f'{percentage:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#--------   Plot Univariate Distributions   --------#\n",
        "\n",
        "def plot_univariate_distributions(X, categorical_columns):\n",
        "    numeric_columns = X.select_dtypes(include=[np.number]).columns\n",
        "    sns.set(style=\"ticks\")\n",
        "    sns.set_context('talk')\n",
        "\n",
        "    # Plot histograms and KDE plots for numeric features\n",
        "    for col in numeric_columns:\n",
        "        plt.figure(figsize=(14, 5))\n",
        "\n",
        "        # Histogram and KDE plot\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.histplot(X[col], kde=True, color='forestgreen')\n",
        "        plt.title(f'Distribution of {col}')\n",
        "        plt.xlabel(col)\n",
        "\n",
        "        # Boxplot to detect outliers\n",
        "        plt.subplot(1, 2, 2)\n",
        "        sns.boxplot(x=X[col], color='limegreen')\n",
        "        plt.title(f'Boxplot of {col}')\n",
        "        plt.xlabel(col)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    # Plot bar plots for categorical features\n",
        "    for col in categorical_columns:\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        sns.countplot(x=X[col], hue=X[col], palette='viridis')\n",
        "        plt.title(f'Category Distribution of {col}')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Count')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"eda_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to eda_utils.py\")\n",
        "# Reload script to make functions available for use\n",
        "import importlib\n",
        "import eda_utils\n",
        "importlib.reload(eda_utils)\n",
        "\n",
        "from eda_utils import *"
      ],
      "metadata": {
        "id": "L3agnq1LJ4-i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97dbc4d8-b7f1-4093-cc72-6f164150be80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to eda_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loan Data Utils"
      ],
      "metadata": {
        "id": "3o1VV2ccDa1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "script_content=r'''\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Define primary, bill, and pay columns\n",
        "primary_columns = ['limit_bal', 'sex', 'education', 'marriage', 'age']\n",
        "bill_columns = ['bill_amt_4_april', 'bill_amt_5_may', 'bill_amt_6_june', 'bill_amt_7_july', 'bill_amt_8_august', 'bill_amt_9_september']\n",
        "pay_columns = ['pay_amt_4_april', 'pay_amt_5_may', 'pay_amt_6_june', 'pay_amt_7_july', 'pay_amt_8_august', 'pay_amt_9_september']\n",
        "ordinal_columns = ['education', 'pay_delay_9_september', 'pay_delay_8_august', 'pay_delay_7_july', 'pay_delay_6_june', 'pay_delay_5_may', 'pay_delay_4_april']\n",
        "\n",
        "def load_data_from_url(url):\n",
        "    try:\n",
        "        df = pd.read_excel(url, header=1)\n",
        "        logging.info(\"Data loaded successfully from URL.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading data from URL: {e}\")\n",
        "        return None\n",
        "    return df\n",
        "\n",
        "def split_features_target(df, target):\n",
        "    try:\n",
        "        X = df.drop(columns=[target])\n",
        "        y = df[target]\n",
        "        logging.info(\"Features and target split successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error splitting features and target: {e}\")\n",
        "        return None, None\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def clean_column_names(df):\n",
        "    try:\n",
        "        df.columns = [col.lower().replace(' ', '_') for col in df.columns]\n",
        "        logging.info(\"Column names cleaned successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error cleaning column names: {e}\")\n",
        "    return df\n",
        "\n",
        "def remove_id_column(df):\n",
        "    if 'id' in df.columns:\n",
        "        df = df.drop(columns=['id'])\n",
        "        logging.info(\"ID column removed.\")\n",
        "    return df\n",
        "\n",
        "def process_sex_column(df):\n",
        "    try:\n",
        "        if 'sex' in df.columns:\n",
        "            df['sex'] = df['sex'].replace({1: 'Male', 2: 'Female'})\n",
        "            df['sex'] = df['sex'].astype('category')\n",
        "            logging.info(\"Sex column processed and converted to categorical successfully.\")\n",
        "        else:\n",
        "            logging.warning(\"Sex column not found in DataFrame.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing sex column: {e}\")\n",
        "    return df\n",
        "\n",
        "def process_marriage_column(df):\n",
        "    try:\n",
        "        if 'marriage' in df.columns:\n",
        "            df['marriage'] = df['marriage'].replace({0: 'Unknown/Others', 3: 'Unknown/Others'})\n",
        "            df['marriage'] = df['marriage'].replace({1: 'Married', 2: 'Single'})\n",
        "            df['marriage'] = pd.Categorical(df['marriage'], categories=['Married', 'Single', 'Unknown/Others'], ordered=False)\n",
        "            logging.info(\"Marriage column processed and converted to categorical successfully.\")\n",
        "        else:\n",
        "            logging.warning(\"Marriage column not found in DataFrame.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error processing marriage column: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def rename_pay_columns(df):\n",
        "    try:\n",
        "        # Only rename if the original columns exist\n",
        "        if all(col in df.columns for col in ['pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']):\n",
        "            pay_columns_new_names = {\n",
        "                'pay_0': 'pay_delay_9_september',\n",
        "                'pay_2': 'pay_delay_8_august',\n",
        "                'pay_3': 'pay_delay_7_july',\n",
        "                'pay_4': 'pay_delay_6_june',\n",
        "                'pay_5': 'pay_delay_5_may',\n",
        "                'pay_6': 'pay_delay_4_april'\n",
        "            }\n",
        "            df = df.rename(columns=pay_columns_new_names)\n",
        "            logging.info(\"Pay delay columns renamed successfully.\")\n",
        "        else:\n",
        "            logging.warning(\"Some or all of the expected pay columns are missing. Renaming skipped.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error renaming pay delay columns: {e}\")\n",
        "    return df\n",
        "\n",
        "def rename_bill_and_payment_columns(df):\n",
        "    try:\n",
        "        # Correct renaming of bill_amt and pay_amt columns\n",
        "        bill_amt_new_names = {\n",
        "            'bill_amt1': 'bill_amt_9_september',\n",
        "            'bill_amt2': 'bill_amt_8_august',\n",
        "            'bill_amt3': 'bill_amt_7_july',\n",
        "            'bill_amt4': 'bill_amt_6_june',\n",
        "            'bill_amt5': 'bill_amt_5_may',\n",
        "            'bill_amt6': 'bill_amt_4_april'\n",
        "        }\n",
        "\n",
        "        pay_amt_new_names = {\n",
        "            'pay_amt1': 'pay_amt_9_september',\n",
        "            'pay_amt2': 'pay_amt_8_august',\n",
        "            'pay_amt3': 'pay_amt_7_july',\n",
        "            'pay_amt4': 'pay_amt_6_june',\n",
        "            'pay_amt5': 'pay_amt_5_may',\n",
        "            'pay_amt6': 'pay_amt_4_april'\n",
        "        }\n",
        "\n",
        "        df = df.rename(columns={**bill_amt_new_names, **pay_amt_new_names})\n",
        "        logging.info(\"Bill and payment columns renamed successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error renaming bill and payment columns: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def label_pay_columns(df, pay_columns):\n",
        "    try:\n",
        "        # Only label the pay_delay_* columns, not pay_amt_*\n",
        "        pay_labels = {\n",
        "            -2: \"No consumption\",\n",
        "            -1: \"Paid in full\",\n",
        "            0: \"Revolving credit\",\n",
        "            1: \"1 month delay\",\n",
        "            2: \"2 months delay\",\n",
        "            3: \"3 months delay\",\n",
        "            4: \"4 months delay\",\n",
        "            5: \"5 months delay\",\n",
        "            6: \"6 months delay\",\n",
        "            7: \"7 months delay\",\n",
        "            8: \"8 months delay\",\n",
        "            9: \"9+ months delay\"\n",
        "        }\n",
        "\n",
        "        for col in pay_columns:\n",
        "            # Ensure we're only mapping the delay columns and not payment amount columns\n",
        "            if 'pay_delay' in col and col in df.columns:\n",
        "                df[col] = df[col].map(pay_labels)\n",
        "                logging.info(f\"Column {col} labeled successfully.\")\n",
        "            else:\n",
        "                logging.error(f\"Column {col} not found in DataFrame.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error labeling pay columns: {e}\")\n",
        "    return df\n",
        "\n",
        "def convert_pay_columns_to_ordinal(df, pay_columns):\n",
        "    try:\n",
        "        pay_order = [\n",
        "            \"No consumption\",  # -2\n",
        "            \"Paid in full\",    # -1\n",
        "            \"Revolving credit\", # 0\n",
        "            \"1 month delay\",   # 1\n",
        "            \"2 months delay\",  # 2\n",
        "            \"3 months delay\",  # 3\n",
        "            \"4 months delay\",  # 4\n",
        "            \"5 months delay\",  # 5\n",
        "            \"6 months delay\",  # 6\n",
        "            \"7 months delay\",  # 7\n",
        "            \"8 months delay\",  # 8\n",
        "            \"9+ months delay\"  # 9\n",
        "        ]\n",
        "\n",
        "        for col in pay_columns:\n",
        "            # Ensure only delay columns are converted to categorical\n",
        "            if 'pay_delay' in col and col in df.columns:\n",
        "                df[col] = pd.Categorical(df[col], categories=pay_order, ordered=True)\n",
        "                logging.info(f\"Column {col} converted to ordinal successfully.\")\n",
        "            else:\n",
        "                logging.error(f\"Column {col} not found in DataFrame.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting pay columns to ordinal categories: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def convert_ordinal_to_category(df, ordinal_columns):\n",
        "    try:\n",
        "        for col in ordinal_columns:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype('category')\n",
        "                logging.info(f\"Column {col} converted to category successfully.\")\n",
        "            else:\n",
        "                logging.error(f\"Column {col} not found in DataFrame.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting ordinal columns to category: {e}\")\n",
        "    return df\n",
        "\n",
        "def convert_education_to_ordinal(df):\n",
        "    try:\n",
        "        # Explicitly map all values to handle known categories\n",
        "        education_mapping = {\n",
        "            1: 'Graduate School',\n",
        "            2: 'University',\n",
        "            3: 'High School',\n",
        "            4: 'Other/Unknown',\n",
        "            5: 'Other/Unknown',\n",
        "            6: 'Other/Unknown',\n",
        "            0: 'Other/Unknown'  # Handle the 0 value as well\n",
        "        }\n",
        "\n",
        "        df['education'] = df['education'].replace(education_mapping)\n",
        "\n",
        "        # Define the order of education categories\n",
        "        education_order = [\n",
        "            \"Other/Unknown\",    # Grouped 0, 4, 5, 6 together\n",
        "            \"High School\",      # 3\n",
        "            \"University\",       # 2\n",
        "            \"Graduate School\"   # 1\n",
        "        ]\n",
        "\n",
        "        if 'education' in df.columns:\n",
        "            # Convert the education column to a categorical type with the specified order\n",
        "            df['education'] = pd.Categorical(df['education'], categories=education_order, ordered=True)\n",
        "            logging.info(\"Education column converted to ordinal categories successfully.\")\n",
        "        else:\n",
        "            logging.error(\"Education column not found in DataFrame.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting education column to ordinal categories: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Utility and validation functions first\n",
        "def check_column_integrity(df, expected_columns):\n",
        "    missing_columns = [col for col in expected_columns if col not in df.columns]\n",
        "    if missing_columns:\n",
        "        logging.error(f\"Missing columns: {missing_columns}\")\n",
        "    else:\n",
        "        logging.info(\"All expected columns are present.\")\n",
        "\n",
        "def validate_data_types(df, ordinal_columns):\n",
        "    for col in ordinal_columns:\n",
        "        if col in df.columns:\n",
        "            if not pd.api.types.is_categorical_dtype(df[col]):\n",
        "                logging.warning(f\"Column {col} is not correctly set as categorical.\")\n",
        "            else:\n",
        "                logging.info(f\"Column {col} is correctly set as categorical with the following categories: {df[col].cat.categories}\")\n",
        "        else:\n",
        "            logging.error(f\"Column {col} not found in DataFrame.\")\n",
        "    logging.info(\"Data type validation complete.\")\n",
        "\n",
        "def reorder_columns(df):\n",
        "    try:\n",
        "        # Combine primary, bill, and pay columns with other columns\n",
        "        other_columns = [col for col in df.columns if col not in primary_columns + bill_columns + pay_columns]\n",
        "        df = df[primary_columns + bill_columns + pay_columns + other_columns]\n",
        "        logging.info(\"Columns reordered successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reordering columns: {e}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(url, categorical_columns, target):\n",
        "    df = load_data_from_url(url)\n",
        "    if df is not None:\n",
        "        try:\n",
        "            df = clean_column_names(df)\n",
        "            df = remove_id_column(df)\n",
        "            df = rename_pay_columns(df)\n",
        "            df = rename_bill_and_payment_columns(df)\n",
        "            df = label_pay_columns(df, [col for col in df.columns if 'pay_delay' in col])\n",
        "            df = convert_pay_columns_to_ordinal(df, [col for col in df.columns if 'pay_delay' in col])\n",
        "            df = reorder_columns(df)\n",
        "            df = convert_education_to_ordinal(df)\n",
        "            df = process_sex_column(df)  # Process the sex column separately\n",
        "            df = process_marriage_column(df)  # Process the marriage column separately\n",
        "            df = convert_ordinal_to_category(df, ordinal_columns)\n",
        "\n",
        "            # Run validation checks\n",
        "            check_column_integrity(df, primary_columns + bill_columns + pay_columns + ordinal_columns)\n",
        "            validate_data_types(df, ordinal_columns)\n",
        "\n",
        "            X, y = split_features_target(df, target)\n",
        "            logging.info(\"Data loaded and preprocessed successfully.\")\n",
        "            return X, y\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in data preprocessing: {e}\")\n",
        "            return None, None\n",
        "    return None, None\n",
        "\n",
        "\n",
        "def check_categorical_order(df, columns):\n",
        "    for col in columns:\n",
        "        if pd.api.types.is_categorical_dtype(df[col]):\n",
        "            print(f\"Column: {col}\")\n",
        "            print(f\"Categories: {df[col].cat.categories}\")\n",
        "            print(f\"Ordered: {df[col].cat.ordered}\\n\")\n",
        "        else:\n",
        "            print(f\"Column: {col} is not categorical.\\n\")\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Write the script to a file\n",
        "with open(\"loan_data_utils.py\", \"w\") as file:\n",
        "    file.write(script_content)\n",
        "\n",
        "print(\"Script successfully written to loan_data_utils.py\")\n",
        "# Reload script to make functions available for use\n",
        "import importlib\n",
        "import loan_data_utils\n",
        "importlib.reload(loan_data_utils)\n",
        "\n",
        "from loan_data_utils import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQZP0omvK50m",
        "outputId": "fb1fbdd1-cc9d-4c8f-e8e9-8b0e1d1714eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Script successfully written to loan_data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mVThgsFklplS"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}